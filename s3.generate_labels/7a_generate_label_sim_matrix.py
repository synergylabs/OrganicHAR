#%%

import os
import glob
import json
import argparse
import sys
from datetime import datetime
from typing import List, Dict, Any
import pandas as pd
import numpy as np
import dotenv
dotenv.load_dotenv()
PROJECT_ROOT = os.environ.get("PROJECT_ROOT")
sys.path.append(PROJECT_ROOT)
# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from src.label_discovery.GlobalSemanticModelEvaluator import GlobalSemanticModelEvaluator
from src.label_discovery.SemanticActivityClustering import find_optimal_activity_clusters

# Base directories - adjust these paths as needed
base_processed_dir = os.environ.get("BASE_PROCESSED_DIR", "./processed_data")
base_results_dir = os.environ.get("BASE_RESULTS_DIR", "./results")

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Generate label similarity matrices for each location cluster")
    
    # Required arguments
    parser.add_argument("--participant", default="eu2a2-data-collection", help="Participant name (e.g., P5data-collection)")
    parser.add_argument("--session_prefix", default="P12", help="Session prefix")
    
    # Optional arguments
    parser.add_argument("--window_size", default="5.0", help="Window size in seconds (default: 5.0)")
    parser.add_argument("--sliding_window_length", default="0.5", help="Sliding window length (default: 0.5)")
    parser.add_argument("--camera_type", choices=["birdseye", "depth"], default="birdseye",
                       help="Camera type to process (default: birdseye)")
    parser.add_argument("--description_vlm_model", required=True, help="VLM model used for description generation (default: gpt-4.1-mini)")
    parser.add_argument("--clustering_vlm_model", default="gpt-4.1", help="VLM model used for location and activity clustering (default: gpt-4.1)")
    parser.add_argument("--alpha", required=True, type=float, help="Alpha for activity clustering (default: 0.5)")
    parser.add_argument("--max_clusters", required=True, type=int, help="Max clusters for activity clustering (default: 10)")
    parser.add_argument("--debug", action="store_true", help="Debug mode")
    
    return parser.parse_args()

def get_session_data(base_processed_dir, participant, session_prefix, window_size, sliding_window_length):
    """Get session data similar to 6c_generate_location_action_labels.py"""
    # get all session information
    watch_sessions_file = f"{base_processed_dir}/{participant}/watch_sessions_filtered.json"
    if not os.path.exists(watch_sessions_file):
        print(f"File {watch_sessions_file} does not exist. Exiting...")
        exit(1)
    with open(watch_sessions_file, "r") as f:
        watch_sessions = json.load(f)
    
    # get the video and depth directories
    base_video_dir = f"{base_processed_dir}/{participant}/processed_video_data"
    base_depth_dir = f"{base_processed_dir}/{participant}/processed_depth_data"

    def video_file_exists(base_filename, video_dir, depth_dir):
        # check if the file exists in the directory
        if os.path.exists(os.path.join(video_dir, base_filename)):
            return True
        if os.path.exists(os.path.join(depth_dir, base_filename)):
            return True
        return False

    collected_sessions = [
        {
            "session_key": f"{session_prefix}-{str(session_idx).zfill(2)}-{session_data['start']}_{session_data['end']}",
            **session_data,
        }
        for session_idx, session_data in enumerate(watch_sessions)
        if video_file_exists(f"{session_prefix}-{str(session_idx).zfill(2)}-{session_data['start']}_{session_data['end']}.mp4", base_video_dir, base_depth_dir)
    ]
    return collected_sessions

def  load_location_action_data(args, session_key):
    """Load location-action data from the previous step (6c_generate_location_action_labels.py)"""
    # Setup directories based on 6c_generate_location_action_labels.py structure
    label_generation_base_dir = f"{base_results_dir}/{args.participant}/label_generation_{args.window_size}_{args.sliding_window_length}/{args.description_vlm_model}"
    label_generation_dir = os.path.join(label_generation_base_dir, session_key)
    
    # Load location-action labels CSV file generated by 6c
    location_action_file = os.path.join(label_generation_dir, f"location_activity_stats_{args.clustering_vlm_model}.csv")
    
    if not os.path.exists(location_action_file):
        print(f"Location-action file not found: {location_action_file}")
        print("Please run 6c_generate_location_action_labels.py first to generate location-action labels")
        return None, None
    
    print(f"Loading location-action data from: {location_action_file}")
    df_location_actions = pd.read_csv(location_action_file, dtype=str)
    
    # Filter out rows with empty activity clusters
    df_location_actions = df_location_actions[~df_location_actions['activity_cluster'].isnull()]
    df_location_actions = df_location_actions[df_location_actions['activity_cluster'] != '']
    df_location_actions = df_location_actions[df_location_actions['activity_cluster'] != 'nan']

    # filter out rows where location cluster is no_match
    df_location_actions = df_location_actions[df_location_actions['location_cluster'] != 'no_match']

    
    print(f"Loaded {len(df_location_actions)} location-action records")
    print(f"Unique locations: {df_location_actions['location_cluster'].nunique()}")
    print(f"Unique activities: {df_location_actions['activity_cluster'].nunique()}")
    
    return df_location_actions, label_generation_dir

def generate_similarity_matrices(df_location_actions, label_generation_dir, clustering_vlm_model):
    """Generate similarity matrices for each location using SemanticModelEvaluator"""
    
    # Create similarity cache directory
    similarity_cache_dir = os.path.join(label_generation_dir, f"similarity_cache_{clustering_vlm_model}")
    os.makedirs(similarity_cache_dir, exist_ok=True)


    # get all the location activity pairs
    location_activity_pairs = df_location_actions[['location_cluster', 'activity_cluster']].values.tolist()
    location_activity_pairs = list(set([tuple(pair) for pair in location_activity_pairs]))
    
    # Initialize GlobalSemanticModelEvaluator
    global_evaluator = GlobalSemanticModelEvaluator(
        activities=location_activity_pairs,
        cache_dir=similarity_cache_dir,
    )

    # Compute similarity matrix
    similarity_matrix = global_evaluator.similarity_matrix

    # save the similarity matrix
    similarity_matrix.to_csv(os.path.join(similarity_cache_dir, "similarity_matrix.csv"))

    return similarity_matrix, global_evaluator


def save_similarity_results(args, session_key, session_data, sim_df, optimal_activity_clusters, global_evaluator, 
                          df_location_actions, label_generation_dir, clustering_vlm_model):
    """Save similarity matrix results globally"""
    
    # Create similarity results directory
    similarity_results_dir = os.path.join(label_generation_dir, f"similarity_results_{clustering_vlm_model}")
    os.makedirs(similarity_results_dir, exist_ok=True)
    
    # Save global similarity matrix
    sim_file = os.path.join(label_generation_dir, "global_similarity_matrix.csv")
    sim_df.to_csv(sim_file)
    
    print(f"Saved global similarity matrix: {sim_file}")

    # Save optimal activity clusters
    cluster_file = os.path.join(label_generation_dir, f"optimal_activity_clusters_{args.alpha}.csv")
    optimal_activity_clusters.to_csv(cluster_file)
    
    
    # Save summary statistics for the global matrix
    summary_stats = {
        'semantic_similarity': {
            'num_activities': len(optimal_activity_clusters),
            'similarity_matrix_shape': sim_df.shape,
            'avg_similarity': float(np.mean(sim_df.values)),
            'min_similarity': float(np.min(sim_df.values)),
            'max_similarity': float(np.max(sim_df.values))
        },
        'semantic_activity_clusters': {
            'num_clusters': optimal_activity_clusters.cluster_id.nunique(),
            'cluster_sizes': optimal_activity_clusters.merged_label.value_counts().to_dict(),
            'raw_activity_clusters': optimal_activity_clusters.to_dict(orient='records'),
        }
    }
    
    stats_file = os.path.join(similarity_results_dir, "similarity_statistics.json")
    with open(stats_file, "w") as f:
        json.dump(summary_stats, f, indent=2)
    
    # Save overall summary
    summary = {
        'participant': args.participant,
        'session_prefix': args.session_prefix,
        'session_key': session_key,
        'session_start': session_data['start'],
        'session_end': session_data['end'],
        'camera_type': args.camera_type,
        'window_size': args.window_size,
        'sliding_window_length': args.sliding_window_length,
        'description_vlm_model': args.description_vlm_model,
        'clustering_vlm_model': clustering_vlm_model,
        'total_records_processed': int(df_location_actions['count'].astype(int).sum()),
        'total_activities_processed': len(global_evaluator.activities),
        'files': {
            'global_similarity_matrix': sim_file,
            'similarity_statistics': stats_file,
            'optimal_activity_clusters': cluster_file
        }
    }
    
    summary_file = os.path.join(similarity_results_dir, f"similarity_matrix_summary_{clustering_vlm_model}_{session_key}.json")
    with open(summary_file, "w") as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nSimilarity matrix results saved:")
    print(f"  Results directory: {similarity_results_dir}")
    print(f"  Global similarity matrix: {sim_file}")
    print(f"  Statistics: {stats_file}")
    print(f"  Summary: {summary_file}")
    
    return summary

def main():
    """Main function"""
    args = parse_args()
    
    print(f"Starting similarity matrix generation for participant: {args.participant}")
    print(f"Session prefix: {args.session_prefix}")
    print(f"Camera type: {args.camera_type}")
    print(f"Window parameters: {args.window_size}s window, {args.sliding_window_length}s step")
    print(f"Description VLM Model: {args.description_vlm_model}")
    print(f"Clustering VLM Model: {args.clustering_vlm_model}")
    
    # Get session data
    collected_sessions = get_session_data(base_processed_dir, args.participant, args.session_prefix, args.window_size, args.sliding_window_length)
    
    if not collected_sessions:
        print("No sessions found. Exiting...")
        return 1
    
    # Loop over individual sessions
    for session_index, session_data in enumerate(collected_sessions):
        session_key = session_data["session_key"]
        
        print(f"\n=== Processing session {session_index + 1}: {session_key} ===")
        
        # Load location-action data from previous step
        df_location_actions, label_generation_dir = load_location_action_data(args, session_key)
        if df_location_actions is None:
            print(f"No location-action data found for session {session_key}. Skipping...")
            continue
        
        # Check if results already exist
        summary_file = os.path.join(label_generation_dir, f"global_similarity_matrix.csv")
        cluster_file = os.path.join(label_generation_dir, f"optimal_activity_clusters_{args.alpha}.csv")
        if os.path.exists(summary_file) and os.path.exists(cluster_file) and not args.debug:
            print(f"Similarity matrix results for {session_key} already exist. Skipping...")
            continue
        
        # Generate similarity matrices
        sim_df, global_evaluator = generate_similarity_matrices(
            df_location_actions, label_generation_dir, args.clustering_vlm_model
        )
        
        if sim_df is None:
            print(f"No similarity matrix generated for session {session_key}. Skipping...")
            continue


        # find the optimal activity clusters
        optimal_activity_clusters = find_optimal_activity_clusters(
            sim_df, df_location_actions, alpha=args.alpha, max_clusters=args.max_clusters
        )
        
        # Save results
        summary = save_similarity_results(
            args, session_key, session_data, sim_df, optimal_activity_clusters, global_evaluator,
            df_location_actions, label_generation_dir, args.clustering_vlm_model
        )
        
        print(f"âœ“ Completed similarity matrix generation for session {session_key}")
        print(f"  Processed {len(global_evaluator.activities)} activities")
    
    print("\nSimilarity matrix generation completed!")
    return 0

if __name__ == "__main__":
    exit(main())


