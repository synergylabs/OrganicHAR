import os
import glob
import json
import argparse
import sys
from datetime import datetime
from typing import List, Dict, Any
import pandas as pd
import dotenv

dotenv.load_dotenv()
# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from src.common.video_processing.LocationActivityClustering import LocationActivityClustering

# Base directories - adjust these paths as needed
base_processed_dir = os.environ.get("BASE_PROCESSED_DIR", "./processed_data")
base_results_dir = "/Volumes/Research-Prasoon/OrganicHAR/inhome_evaluation"

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Generate activity labels for each location cluster")
    
    # Required arguments
    parser.add_argument("--participant", default="P5data-collection", help="Participant name (e.g., P5data-collection)")
    parser.add_argument("--session_prefix", default="P11", help="Session prefix")
    
    # Optional arguments
    parser.add_argument("--window_size", default="5.0", help="Window size in seconds (default: 5.0)")
    parser.add_argument("--sliding_window_length", default="0.5", help="Sliding window length (default: 0.5)")
    parser.add_argument("--camera_type", choices=["birdseye", "depth"], default="birdseye",
                       help="Camera type to process (default: birdseye)")
    parser.add_argument("--description_vlm_model", default="gpt-4.1", help="VLM model used for description generation (default: gpt-4.1)")
    parser.add_argument("--clustering_vlm_model", default="gpt-4.1", help="VLM model used for location and activity clustering (default: gpt-4.1)")
    parser.add_argument("--debug", action="store_true", help="Debug mode")
    
    return parser.parse_args()

def get_session_data(base_processed_dir, participant, session_prefix, window_size, sliding_window_length):
    """Get session data similar to 5a_cluster_data.py"""
    # get all session information
    watch_sessions_file = f"{base_processed_dir}/{participant}/watch_sessions_filtered.json"
    if not os.path.exists(watch_sessions_file):
        print(f"File {watch_sessions_file} does not exist. Exiting...")
        exit(1)
    with open(watch_sessions_file, "r") as f:
        watch_sessions = json.load(f)
    
    # get the video and depth directories
    base_video_dir = f"{base_processed_dir}/{participant}/processed_video_data"
    base_depth_dir = f"{base_processed_dir}/{participant}/processed_depth_data"

    def video_file_exists(base_filename, video_dir, depth_dir):
        # check if the file exists in the directory
        if os.path.exists(os.path.join(video_dir, base_filename)):
            return True
        if os.path.exists(os.path.join(depth_dir, base_filename)):
            return True
        return False

    collected_sessions = [
        {
            "session_key": f"{session_prefix}-{str(session_idx).zfill(2)}-{session_data['start']}_{session_data['end']}",
            **session_data,
        }
        for session_idx, session_data in enumerate(watch_sessions)
        if video_file_exists(f"{session_prefix}-{str(session_idx).zfill(2)}-{session_data['start']}_{session_data['end']}.mp4", base_video_dir, base_depth_dir)
    ]
    return collected_sessions

def load_locations_data(args, session_key):
    """Load locations data from the previous step (6b_generate_location_labels.py)"""
    # Setup directories based on 6b_generate_location_labels.py structure
    label_generation_base_dir = f"{base_results_dir}/{args.participant}/label_generation_{args.window_size}_{args.sliding_window_length}/{args.description_vlm_model}"
    label_generation_dir = os.path.join(label_generation_base_dir, session_key)
    
    # Load locations CSV file generated by 6b
    locations_file = os.path.join(label_generation_dir, f"locations_{args.clustering_vlm_model}.csv")
    
    if not os.path.exists(locations_file):
        print(f"Locations file not found: {locations_file}")
        print("Please run 6b_generate_location_labels.py first to generate location labels")
        return None, None
    
    print(f"Loading locations data from: {locations_file}")
    df_locations = pd.read_csv(locations_file)
    df_locations['raw_actions'] = df_locations['action_at_location']
    
    # Check if raw_actions column exists
    if 'raw_actions' not in df_locations.columns:
        print(f"Error: 'raw_actions' column not found in {locations_file}")
        print(f"Available columns: {df_locations.columns.tolist()}")
        return None, None
    
    print(f"Loaded {len(df_locations)} location records")
    return df_locations, label_generation_dir

def cluster_activities_by_location(df_locations, label_generation_dir, clustering_vlm_model):
    """Cluster activities for each location using LocationActivityClustering"""
    
    activity_map_file = os.path.join(label_generation_dir, f"activity_map_{clustering_vlm_model}.json")
    
    if os.path.exists(activity_map_file):
        print(f"Loading existing activity map from: {activity_map_file}")
        with open(activity_map_file, "r") as f:
            location_activity_map = json.load(f)
    else:
        print("Creating new activity clusters...")
        location_merged_activities = {}
        location_activity_map = {}
        location_activities = {}
        
        for location_cluster in df_locations['location_cluster'].unique():
            # Skip no_match locations
            if ("no" in location_cluster) and ("match" in location_cluster):
                print(f"Skipping location_cluster: {location_cluster}")
                continue
            
            print(f"\n=== Processing location cluster: {location_cluster} ===")
            
            # Initialize activity clustering
            activity_clusterer = LocationActivityClustering(model_name=clustering_vlm_model)
            
            # Get activities for this location
            location_activities[location_cluster] = df_locations[
                df_locations['location_cluster'] == location_cluster
            ]['raw_actions'].values.tolist()
            
            # Get unique activities and their counts
            activity_value_counts = pd.Series(location_activities[location_cluster]).value_counts().to_dict()
            unique_raw_activities = sorted(list(activity_value_counts.keys()))
            
            print(f"Total unique activities found ({location_cluster}): {len(unique_raw_activities)}")
            
            if len(unique_raw_activities) == 0:
                print(f"No activities found for location: {location_cluster}")
                continue
            
            # Cluster activities
            clustered_activities = activity_clusterer.cluster_activities(location_cluster, unique_raw_activities)
            
            print(f"Total merged activities found ({location_cluster}): {len(clustered_activities)}")
            print(f"Merged Activities ({location_cluster}): {list(clustered_activities.keys())}")
            
            for merged_activity in clustered_activities:
                print(f"Activities in {merged_activity}: {clustered_activities[merged_activity]}")
            
            location_merged_activities[location_cluster] = clustered_activities
            activity_clusters = list(clustered_activities.keys())
            
            # Create initial activity cluster map
            activity_cluster_map = {}
            for merged_activity in clustered_activities:
                for activity in clustered_activities[merged_activity]:
                    activity_cluster_map[activity] = merged_activity
            
            # Handle new activities that don't fit existing clusters
            new_activities = []
            for activity in unique_raw_activities:
                if activity not in activity_cluster_map:
                    new_activities.append(activity)
            
            # Batch process new activities
            max_batch_size = 100
            while len(new_activities) > 0:
                print(f"Total new activities found: {len(new_activities)}")
                new_activity_map = activity_clusterer.match_activities_batch(
                    new_activities[:max_batch_size], 
                    activity_clusters
                )
                
                found_new_activity = []
                for activity_str in new_activities[:]:  # Create copy to iterate
                    if activity_str in new_activity_map:
                        activity_cluster_map[activity_str] = new_activity_map[activity_str]
                        found_new_activity.append(activity_str)
                        new_activities.remove(activity_str)
                
                if len(found_new_activity) == 0:
                    for activity_str in new_activities:
                        activity_cluster_map[activity_str] = "no_match"
                    break
            
            location_activity_map[location_cluster] = activity_cluster_map
        
        # Save activity clustering results
        print("Saving activity clustering results...")
        
        raw_activities_file = os.path.join(label_generation_dir, f"raw_activities_{clustering_vlm_model}.json")
        with open(raw_activities_file, "w") as f:
            json.dump(location_activities, f, indent=2)
        
        merged_activities_file = os.path.join(label_generation_dir, f"merged_activities_{clustering_vlm_model}.json")
        with open(merged_activities_file, "w") as f:
            json.dump(location_merged_activities, f, indent=2)
        
        with open(activity_map_file, "w") as f:
            json.dump(location_activity_map, f, indent=2)
        
        print(f"Activity clustering results saved:")
        print(f"  Raw activities: {raw_activities_file}")
        print(f"  Merged activities: {merged_activities_file}")
        print(f"  Activity map: {activity_map_file}")
    
    return location_activity_map

def apply_activity_clusters(df_locations, location_activity_map):
    """Apply activity cluster mapping to the dataframe"""
    
    def get_activity_cluster(orig_action, location_cluster):
        """Get activity cluster for given action and location"""
        action_cluster = location_activity_map.get(location_cluster, {}).get(orig_action, "")
        if ("no" in action_cluster) and ("match" in action_cluster):
            return ""
        return action_cluster
    
    # Apply activity clustering
    df_locations['activity_cluster'] = df_locations.apply(
        lambda x: get_activity_cluster(x['raw_actions'], x['location_cluster']), 
        axis=1
    )
    
    # Clean up activity cluster names
    df_locations['activity_cluster'] = df_locations['activity_cluster'].fillna('')
    df_locations['activity_cluster'] = df_locations['activity_cluster'].astype(str)
    df_locations['activity_cluster'] = df_locations['activity_cluster'].apply(lambda x: x.replace(' ', '_'))
    df_locations['activity_cluster'] = df_locations['activity_cluster'].apply(lambda x: x.replace('-', '_'))
    df_locations['activity_cluster'] = df_locations['activity_cluster'].apply(lambda x: x.replace('/', '_'))
    
    return df_locations

def save_activity_results(args, session_key, session_data, df_locations, label_generation_dir, clustering_vlm_model):
    """Save activity clustering results"""
    
    # Save final location-action labels CSV
    final_labels_file = os.path.join(label_generation_dir, f"location_action_labels_{clustering_vlm_model}.csv")
    df_locations.to_csv(final_labels_file, index=False)
    
    # Create summary statistics
    location_activity_stats = df_locations.groupby(['location_cluster', 'activity_cluster']).size().reset_index(name='count')
    stats_file = os.path.join(label_generation_dir, f"location_activity_stats_{clustering_vlm_model}.csv")
    location_activity_stats.to_csv(stats_file, index=False)
    
    # Save summary
    summary = {
        'participant': args.participant,
        'session_prefix': args.session_prefix,
        'session_key': session_key,
        'session_start': session_data['start'],
        'session_end': session_data['end'],
        'camera_type': args.camera_type,
        'window_size': args.window_size,
        'sliding_window_length': args.sliding_window_length,
        'description_vlm_model': args.description_vlm_model,
        'clustering_vlm_model': clustering_vlm_model,
        'processing_time': datetime.now().isoformat(),
        'total_records': len(df_locations),
        'unique_locations': df_locations['location_cluster'].nunique(),
        'unique_activities': df_locations['activity_cluster'].nunique(),
        'files': {
            'location_action_labels': final_labels_file,
            'location_activity_stats': stats_file
        }
    }
    
    summary_file = os.path.join(label_generation_dir, f"activity_clustering_summary_{clustering_vlm_model}_{session_key}.json")
    with open(summary_file, "w") as f:
        json.dump(summary, f, indent=2)
    
    print(f"Activity clustering results saved:")
    print(f"  Location-Action labels: {final_labels_file}")
    print(f"  Statistics: {stats_file}")
    print(f"  Summary: {summary_file}")

def main():
    """Main function"""
    args = parse_args()
    
    print(f"Starting activity label generation for participant: {args.participant}")
    print(f"Session prefix: {args.session_prefix}")
    print(f"Camera type: {args.camera_type}")
    print(f"Window parameters: {args.window_size}s window, {args.sliding_window_length}s step")
    print(f"Description VLM Model: {args.description_vlm_model}")
    print(f"Clustering VLM Model: {args.clustering_vlm_model}")
    
    # Get session data
    collected_sessions = get_session_data(base_processed_dir, args.participant, args.session_prefix, args.window_size, args.sliding_window_length)
    
    if not collected_sessions:
        print("No sessions found. Exiting...")
        return 1
    
    # Loop over individual sessions
    for session_index, session_data in enumerate(collected_sessions):
        session_key = session_data["session_key"]
        
        print(f"\n=== Processing session {session_index + 1}: {session_key} ===")
        
        # Load locations data from previous step
        df_locations, label_generation_dir = load_locations_data(args, session_key)
        if df_locations is None:
            print(f"No locations data found for session {session_key}. Skipping...")
            continue
        
        # Check if results already exist
        final_labels_file = os.path.join(label_generation_dir, f"location_action_labels_{args.clustering_vlm_model}.csv")
        if os.path.exists(final_labels_file) and not args.debug:
            print(f"Activity clustering results for {session_key} already exist. Skipping...")
            continue
        
        # Cluster activities by location
        location_activity_map = cluster_activities_by_location(df_locations, label_generation_dir, args.clustering_vlm_model)
        
        if not location_activity_map:
            print(f"No activities to cluster for session {session_key}. Skipping...")
            continue
        
        # Apply activity clusters to dataframe
        df_locations = apply_activity_clusters(df_locations, location_activity_map)
        
        # Save results
        save_activity_results(args, session_key, session_data, df_locations, label_generation_dir, args.clustering_vlm_model)
        
        print(f"âœ“ Completed activity clustering for session {session_key}")
    
    print("\nActivity label generation completed!")
    return 0

if __name__ == "__main__":
    exit(main())
