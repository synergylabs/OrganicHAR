"""
GT Label Mapping Script for OrganicHAR Pipeline

This script maps VLM-generated merged activity clusters to ground truth labels
using the GTMapper class. It processes final_labels_*.csv files generated by
the 7b_merge_av_labels.py script.

Ground truth labels are loaded from the organized annotations CSV file created
by 9b_organize_annotations.py, ensuring consistency with the evaluation pipeline.
"""

import sys
import os
import pandas as pd
import numpy as np
import json
import argparse
import glob
from datetime import datetime
import dotenv

dotenv.load_dotenv()
sys.path.append(os.environ.get("PROJECT_ROOT"))
from src.evaluation.GTMapper import GTMapper

# Base directories - adjust these paths as needed
base_processed_dir = os.environ.get("BASE_PROCESSED_DIR", "./processed_data")
base_results_dir = os.environ.get("BASE_RESULTS_DIR", "./results")

# Ground truth file path - will be constructed dynamically
# gt_file = "/Volumes/Vax Storage/phase4_results/ubicomp_results_may25/raw_gt_labels.csv"

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Map VLM-generated labels to ground truth labels")
    
    # Required arguments
    parser.add_argument("--participant", default="eu2a2-data-collection", help="Participant name")
    parser.add_argument("--session_prefix", default="P11", help="Session prefix")
    
    # Optional arguments
    parser.add_argument("--window_size", default="5.0", help="Window size in seconds")
    parser.add_argument("--sliding_window_length", default="0.5", help="Sliding window length")
    parser.add_argument("--description_vlm_model", required=True, help="VLM model used for description generation")
    parser.add_argument("--clustering_vlm_model", default="gpt-4.1", help="VLM model used for clustering")
    parser.add_argument("--merging_threshold", default=0.3, help="Merging threshold used")
    parser.add_argument("--alpha", required=True, help="Alpha for activity clustering")
    parser.add_argument("--early_samples", default=60, type=int, help="Number of samples from early sessions")
    parser.add_argument("--middle_samples", default=60, type=int, help="Number of samples from middle sessions")
    parser.add_argument("--late_samples", default=130, type=int, help="Number of samples from late sessions")
    parser.add_argument("--mode", type=str, default='create',
                        choices=['create', 'check', 'compile', 'full'],
                        help='Mode to run the script in')
    parser.add_argument("--debug", action="store_true", help="Debug mode")
    
    return parser.parse_args()

def get_session_data(base_processed_dir, participant, session_prefix, window_size, sliding_window_length):
    """Get session data similar to other pipeline scripts"""
    # get all session information
    watch_sessions_file = f"{base_processed_dir}/{participant}/watch_sessions_filtered.json"
    if not os.path.exists(watch_sessions_file):
        print(f"File {watch_sessions_file} does not exist. Exiting...")
        exit(1)
    with open(watch_sessions_file, "r") as f:
        watch_sessions = json.load(f)
    
    # get the video and depth directories
    base_video_dir = f"{base_processed_dir}/{participant}/processed_video_data"
    base_depth_dir = f"{base_processed_dir}/{participant}/processed_depth_data"

    def video_file_exists(base_filename, video_dir, depth_dir):
        # check if the file exists in the directory
        if os.path.exists(os.path.join(video_dir, base_filename)):
            return True
        if os.path.exists(os.path.join(depth_dir, base_filename)):
            return True
        return False

    collected_sessions = [
        {
            "session_key": f"{session_prefix}-{str(session_idx).zfill(2)}-{session_data['start']}_{session_data['end']}",
            **session_data,
        }
        for session_idx, session_data in enumerate(watch_sessions)
        if video_file_exists(f"{session_prefix}-{str(session_idx).zfill(2)}-{session_data['start']}_{session_data['end']}.mp4", base_video_dir, base_depth_dir)
    ]
    return collected_sessions

def load_ground_truth_labels(args):
    """Load and process ground truth labels from organized annotations file"""
    # Calculate total target samples using args (matching 9b_organize_annotations.py logic)
    total_target_samples = args.early_samples + args.middle_samples + args.late_samples
    
    # Construct path to organized GT file (matching 9b_organize_annotations.py)
    gt_segments_base_dir = f"{base_results_dir}/{args.participant}/gt_segments"
    gt_output_dir = f"{gt_segments_base_dir}/temporal_stratified_{total_target_samples}_samples"
    gt_file = f"{gt_output_dir}/organized_annotations.csv"
    
    if not os.path.exists(gt_file):
        print(f"Organized ground truth file not found: {gt_file}")
        print("Please run 9b_organize_annotations.py first to create the organized GT file.")
        return None
    
    print(f"Loading ground truth labels from: {gt_file}")
    df_gt = pd.read_csv(gt_file)
    
    # Extract unique ground truth labels from the gt_label column
    unique_gt_labels = df_gt['gt_label'].dropna().unique()
    
    # Filter out empty strings
    unique_gt_labels = [label.replace("; ", " AND ") for label in unique_gt_labels if label and str(label).strip()]

    # filter out No Video, No Person, Unknown and  labels
    unique_gt_labels = [label for label in unique_gt_labels if label not in ["No Video", "No Person","Unknown"]]
    
    print(f"Loaded {len(unique_gt_labels)} unique ground truth labels")
    return unique_gt_labels

def create_batches(args, collected_sessions, unique_gt_labels):
    """Create batches for all configurations but don't wait for results."""
    batch_id_prefix = f"gtmap_v3_alpha_{args.alpha}_{args.session_prefix}_{args.description_vlm_model}"
    
    # Initialize the GTMapper with appropriate cache directory
    gt_mapper = GTMapper(model="gpt-4.1", cache_dir="/tmp/gt_mapping_cache_pipeline")
    
    batches_created = []
    
    for session_data in collected_sessions:
        session_key = session_data["session_key"]
        
        print(f"Processing session: {session_key}")
        
        # Find final activity clusters
        label_generation_dir = f"{base_results_dir}/{args.participant}/label_generation_{args.window_size}_{args.sliding_window_length}/{args.description_vlm_model}"
        cluster_file = os.path.join(label_generation_dir, session_key, f"optimal_activity_clusters_{args.alpha}.csv")
        
        # Extract unique VLM labels
        df_activity_clusters = pd.read_csv(cluster_file, index_col=0)

        # get the mapping from every cluster id to the list of activity clusters
        cluster_id_to_activity_clusters = df_activity_clusters.groupby('cluster_id')['activity'].apply(list).to_dict()

        # get the unique activity clusters by convert list to string connected by 'OR'
        unique_vlm_labels = list(set([' OR '.join(sorted(activity_clusters)) for activity_clusters in cluster_id_to_activity_clusters.values()]))
        
        if not unique_vlm_labels:
            print(f"No VLM labels found for session {session_key}")
            continue
        
        # Create batch ID
        batch_id = f"{batch_id_prefix}_{session_key}_{args.merging_threshold}_{args.clustering_vlm_model}"
        
        # Check if batch already exists
        if gt_mapper.batch_exists(batch_id):
            print(f"Batch {batch_id} already exists. Skipping creation...")
            batches_created.append(batch_id)
            continue
        
        print(f"Creating GT mappings batch for {batch_id}...")
        
        try:
            # Create a batch for this configuration
            gt_mapper.create_batch(
                discovered_labels=unique_vlm_labels,
                ground_truth_labels=unique_gt_labels,
                batch_id=batch_id
            )
            batches_created.append(batch_id)
            print(f"Created batch {batch_id}")
        except Exception as e:
            print(f"Error creating batch {batch_id}: {str(e)}")
    
    return batches_created

def compile_results(args, collected_sessions, check_status=True):
    """Compile results for all batches that have completed."""
    batch_id_prefix = f"gtmap_v3_alpha_{args.alpha}_{args.session_prefix}_{args.description_vlm_model}"
    
    # Initialize the GTMapper
    gt_mapper = GTMapper(model="gpt-4.1", cache_dir="/tmp/gt_mapping_cache_pipeline")
    
    compiled_results = {}
    
    for session_data in collected_sessions:
        session_key = session_data["session_key"]
        
        batch_id = f"{batch_id_prefix}_{session_key}_{args.merging_threshold}_{args.clustering_vlm_model}"
        
        # Check if batch exists
        if not gt_mapper.batch_exists(batch_id):
            print(f"Batch {batch_id} does not exist. Skipping compilation...")
            continue
        
        # Check batch status if required
        if check_status:
            _, status = gt_mapper.get_batch_results(batch_id, wait_for_completion=False)
            if status != "completed":
                print(f"Batch {batch_id} is not completed yet. Status: {status}")
                continue
        
        # Get batch results without waiting
        results, status = gt_mapper.get_batch_results(batch_id, wait_for_completion=False)
        
        if status == "completed" and results:
            # Compile all the results
            all_mappings = gt_mapper.compile_all_results(batch_id)
            
            # Format the results for evaluation
            formatted_mappings = {}
            detailed_mappings = {}
            for mapping in all_mappings.get("mappings", []):
                gt_label = mapping.get("ground_truth", "")
                av_mappings = mapping.get("mapping", [])
                reasoning = mapping.get("reasoning", "")
                formatted_mappings[gt_label] = av_mappings
                # Store detailed mapping with reasoning for JSON export
                detailed_mappings[gt_label] = {
                    "mapped_labels": av_mappings,
                    "reasoning": reasoning
                }
            
            
                results_dir = f"{base_results_dir}/{args.participant}/label_generation_{args.window_size}_{args.sliding_window_length}/{args.description_vlm_model}/{session_key}"
                
                # Create suffix for output file
                suffix = f"{args.merging_threshold}_{args.clustering_vlm_model}_alpha_{args.alpha}"
                
                # Save the mapping file
                mappings_file = os.path.join(results_dir, f"gt_mapped_av_{suffix}.csv")
                
                # Convert mappings to DataFrame for easier handling
                mapping_rows = []
                for gt_label, vlm_labels in formatted_mappings.items():
                    for vlm_label in vlm_labels:
                        mapping_rows.append({
                            'ground_truth_label': gt_label,
                            'vlm_generated_label': vlm_label,
                            'session_key': session_key,
                            'merging_threshold': args.merging_threshold,
                            'clustering_vlm_model': args.clustering_vlm_model,
                            'mapping_timestamp': datetime.now().isoformat()
                        })
                
                df_mappings = pd.DataFrame(mapping_rows)
                df_mappings.to_csv(mappings_file, index=False)
                
                # Also save as JSON for reference (with reasoning)
                json_mappings_file = os.path.join(results_dir, f"gt_av_mappings_{suffix}.json")
                with open(json_mappings_file, 'w') as f:
                    json.dump(detailed_mappings, f, indent=2, default=str)
                
                print(f"Saved GT mappings for {batch_id}")
                print(f"  CSV: {mappings_file}")
                print(f"  JSON: {json_mappings_file}")
                
                compiled_results[batch_id] = {
                    "status": "completed",
                    "mappings_csv": mappings_file,
                    "mappings_json": json_mappings_file,
                    "session_key": session_key
                }
            else:
                print(f"Could not find directory for session {session_key}")
                compiled_results[batch_id] = {
                    "status": "error",
                    "error": "Could not find final_labels directory"
                }
        else:
            compiled_results[batch_id] = {
                "status": status
            }
    
    # Save summary of all compilations
    summary_file = f"{base_results_dir}/{args.participant}/gt_mapping_summary_{args.session_prefix}.json"
    os.makedirs(os.path.dirname(summary_file), exist_ok=True)
    with open(summary_file, 'w') as f:
        json.dump(compiled_results, f, indent=2)
    
    return compiled_results

def check_batch_status(args, collected_sessions):
    """Check status of all batches."""
    batch_id_prefix = f"gtmap_v3_alpha_{args.alpha}_{args.session_prefix}_{args.description_vlm_model}"
    
    # Initialize the GTMapper
    gt_mapper = GTMapper(model="gpt-4.1", cache_dir="/tmp/gt_mapping_cache_pipeline")
    
    status_summary = {}
    
    for session_data in collected_sessions:
        session_key = session_data["session_key"]
        
        batch_id = f"{batch_id_prefix}_{session_key}_{args.merging_threshold}_{args.clustering_vlm_model}"
        
        # Check if batch exists
        if not gt_mapper.batch_exists(batch_id):
            continue
        
        # Get batch status
        _, status = gt_mapper.get_batch_results(batch_id, wait_for_completion=False)
        status_summary[batch_id] = status
    
    # Print status summary
    completed = sum(1 for status in status_summary.values() if status == "completed")
    in_progress = sum(1 for status in status_summary.values() if status != "completed")
    print(f"Batch Status Summary {batch_id_prefix}: {completed} completed, {in_progress} in progress")
    
    # Save status summary
    status_file = f"{base_results_dir}/{args.participant}/batch_status_{args.session_prefix}.json"
    os.makedirs(os.path.dirname(status_file), exist_ok=True)
    with open(status_file, 'w') as f:
        json.dump(status_summary, f, indent=2)
    
    return status_summary

def main():
    """Main function"""
    args = parse_args()
    
    print(f"Starting GT label mapping for participant: {args.participant}")
    print(f"Session prefix: {args.session_prefix}")
    print(f"Window parameters: {args.window_size}s window, {args.sliding_window_length}s step")
    print(f"Description VLM Model: {args.description_vlm_model}")
    print(f"Clustering VLM Model: {args.clustering_vlm_model}")
    print(f"Merging threshold: {args.merging_threshold}")
    print(f"Sample counts: {args.early_samples} early, {args.middle_samples} middle, {args.late_samples} late (total: {args.early_samples + args.middle_samples + args.late_samples})")
    print(f"Mode: {args.mode}")
    
    # Load ground truth labels
    unique_gt_labels = load_ground_truth_labels(args)
    if unique_gt_labels is None:
        print("Could not load ground truth labels. Exiting...")
        return 1
    
    # Get session data
    collected_sessions = get_session_data(base_processed_dir, args.participant, args.session_prefix, args.window_size, args.sliding_window_length)
    
    if not collected_sessions:
        print("No sessions found. Exiting...")
        return 1
    
    print(f"Found {len(collected_sessions)} sessions to process")
    
    if args.mode == 'create':
        # Only create batches and exit
        batches = create_batches(args, collected_sessions, unique_gt_labels)
        print(f"Created {len(batches)} batches")
    
    elif args.mode == 'check':
        # Only check status and exit
        status = check_batch_status(args, collected_sessions)
        # print(f"Checked status of {len(status)} batches")
    
    elif args.mode == 'compile':
        # Only compile results without checking status
        results = compile_results(args, collected_sessions, check_status=False)
        print(f"Compiled results for {len(results)} batches")
    
    elif args.mode == 'full':
        # Run the full pipeline
        batches = create_batches(args, collected_sessions, unique_gt_labels)
        status = check_batch_status(args, collected_sessions)
        results = compile_results(args, collected_sessions)
        print(f"Processed {len(batches)} batches, with {len([r for r in results.values() if r.get('status') == 'completed'])} completed")
    
    return 0

if __name__ == "__main__":
    exit(main()) 